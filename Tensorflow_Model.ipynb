{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "31577c06",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Size of dataset= 24344255\n",
      "Epoch 1/10\n",
      "7161/7161 [==============================] - 9s 1ms/step - loss: 8606.3359 - accuracy: 0.4954\n",
      "Epoch 2/10\n",
      "7161/7161 [==============================] - 9s 1ms/step - loss: 7539.3115 - accuracy: 0.4995\n",
      "Epoch 3/10\n",
      "7161/7161 [==============================] - 9s 1ms/step - loss: 5401.8184 - accuracy: 0.5347\n",
      "Epoch 4/10\n",
      "7161/7161 [==============================] - 9s 1ms/step - loss: 4641.9155 - accuracy: 0.5544\n",
      "Epoch 5/10\n",
      "7161/7161 [==============================] - 9s 1ms/step - loss: 4132.1201 - accuracy: 0.5716\n",
      "Epoch 6/10\n",
      "7161/7161 [==============================] - 9s 1ms/step - loss: 2379.4509 - accuracy: 0.6270\n",
      "Epoch 7/10\n",
      "7161/7161 [==============================] - 9s 1ms/step - loss: 2190.8511 - accuracy: 0.6519\n",
      "Epoch 8/10\n",
      "7161/7161 [==============================] - 9s 1ms/step - loss: 1682.0312 - accuracy: 0.6951\n",
      "Epoch 9/10\n",
      "7161/7161 [==============================] - 9s 1ms/step - loss: 1061.2218 - accuracy: 0.7505\n",
      "Epoch 10/10\n",
      "7161/7161 [==============================] - 9s 1ms/step - loss: 919.8734 - accuracy: 0.7852\n",
      "1791/1791 [==============================] - 1s 746us/step\n",
      "1791/1791 [==============================] - 2s 963us/step - loss: 199.6570 - accuracy: 0.9136\n",
      "Accuracy: 0.9135839343070984\n",
      "Precision: 0.9918024928092042\n",
      "Recall: 0.8123527564001884\n",
      "F1-Score: 0.8931531687100673\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Found untraced functions such as _update_step_xla while saving (showing 1 of 1). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: tensorflow_model/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: tensorflow_model/assets\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['metrics.pkl']"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import f1_score, precision_score, recall_score\n",
    "import joblib\n",
    "import pandas as pd\n",
    "\n",
    "# Load the dataset\n",
    "data = pd.read_csv('portScan.csv')\n",
    "X = data[['Flow Bytes/s', 'Total Length of Fwd Packets', 'Fwd IAT Total', 'Flow Duration']]\n",
    "# y = data['Label']\n",
    "\n",
    "# data.fillna(0)\n",
    "print(\"Size of dataset=\",data.size)\n",
    "attack_or_not=[]\n",
    "for i in data['Label']: #it changes the normal label to \"1\" and the attack tag to \"0\" for use in the machine learning algorithm\n",
    "    if i ==\"BENIGN\":\n",
    "        attack_or_not.append(1)\n",
    "    else:\n",
    "        attack_or_not.append(0)     \n",
    "        # attack_or_not.append(1) \n",
    "data['Label']=attack_or_not\n",
    "\n",
    "y = data['Label']\n",
    "\n",
    "# Split the data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Define the TensorFlow model\n",
    "model = tf.keras.Sequential([\n",
    "    tf.keras.layers.Dense(64, activation='relu', input_shape=(4,)),\n",
    "    tf.keras.layers.Dense(1, activation='sigmoid')\n",
    "])\n",
    "\n",
    "# Compile the model\n",
    "model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "# Train the model\n",
    "model.fit(X_train, y_train, epochs=10, batch_size=32)\n",
    "\n",
    "# Evaluate the model\n",
    "y_pred = model.predict(X_test)\n",
    "y_pred_binary = [1 if pred >= 0.5 else 0 for pred in y_pred]\n",
    "\n",
    "accuracy = model.evaluate(X_test, y_test)[1]\n",
    "precision = precision_score(y_test, y_pred_binary)\n",
    "recall = recall_score(y_test, y_pred_binary)\n",
    "f1 = f1_score(y_test, y_pred_binary)\n",
    "\n",
    "# Print the evaluation metrics\n",
    "print(\"Accuracy:\", accuracy)\n",
    "print(\"Precision:\", precision)\n",
    "print(\"Recall:\", recall)\n",
    "print(\"F1-Score:\", f1)\n",
    "\n",
    "# Save the model\n",
    "model.save('tensorflow_model')\n",
    "\n",
    "# Save the metrics\n",
    "metrics = {'accuracy': accuracy, 'precision': precision, 'recall': recall, 'f1-score': f1}\n",
    "joblib.dump(metrics, 'metrics.pkl')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "1259b29e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5/5 [==============================] - 0s 2ms/step\n",
      "\n",
      "\n",
      "\n",
      "**************************************************************************************************************\n",
      "[[1.        ]\n",
      " [1.        ]\n",
      " [1.        ]\n",
      " [1.        ]\n",
      " [0.03437895]\n",
      " [1.        ]\n",
      " [1.        ]\n",
      " [1.        ]\n",
      " [1.        ]\n",
      " [1.        ]\n",
      " [1.        ]\n",
      " [       nan]\n",
      " [1.        ]\n",
      " [1.        ]\n",
      " [1.        ]\n",
      " [1.        ]\n",
      " [1.        ]\n",
      " [1.        ]\n",
      " [1.        ]\n",
      " [1.        ]\n",
      " [1.        ]\n",
      " [1.        ]\n",
      " [1.        ]\n",
      " [       nan]\n",
      " [1.        ]\n",
      " [1.        ]\n",
      " [1.        ]\n",
      " [1.        ]\n",
      " [1.        ]\n",
      " [1.        ]\n",
      " [1.        ]\n",
      " [1.        ]\n",
      " [1.        ]\n",
      " [1.        ]\n",
      " [1.        ]\n",
      " [1.        ]\n",
      " [1.        ]\n",
      " [1.        ]\n",
      " [1.        ]\n",
      " [1.        ]\n",
      " [1.        ]\n",
      " [1.        ]\n",
      " [1.        ]\n",
      " [1.        ]\n",
      " [0.        ]\n",
      " [1.        ]\n",
      " [1.        ]\n",
      " [1.        ]\n",
      " [1.        ]\n",
      " [1.        ]\n",
      " [1.        ]\n",
      " [1.        ]\n",
      " [1.        ]\n",
      " [1.        ]\n",
      " [1.        ]\n",
      " [1.        ]\n",
      " [1.        ]\n",
      " [1.        ]\n",
      " [1.        ]\n",
      " [1.        ]\n",
      " [1.        ]\n",
      " [1.        ]\n",
      " [1.        ]\n",
      " [1.        ]\n",
      " [1.        ]\n",
      " [1.        ]\n",
      " [1.        ]\n",
      " [1.        ]\n",
      " [1.        ]\n",
      " [1.        ]\n",
      " [1.        ]\n",
      " [1.        ]\n",
      " [1.        ]\n",
      " [1.        ]\n",
      " [1.        ]\n",
      " [1.        ]\n",
      " [1.        ]\n",
      " [1.        ]\n",
      " [1.        ]\n",
      " [1.        ]\n",
      " [1.        ]\n",
      " [1.        ]\n",
      " [1.        ]\n",
      " [1.        ]\n",
      " [1.        ]\n",
      " [1.        ]\n",
      " [1.        ]\n",
      " [1.        ]\n",
      " [1.        ]\n",
      " [1.        ]\n",
      " [1.        ]\n",
      " [1.        ]\n",
      " [1.        ]\n",
      " [1.        ]\n",
      " [1.        ]\n",
      " [1.        ]\n",
      " [1.        ]\n",
      " [1.        ]\n",
      " [1.        ]\n",
      " [0.        ]\n",
      " [1.        ]\n",
      " [1.        ]\n",
      " [1.        ]\n",
      " [1.        ]\n",
      " [1.        ]\n",
      " [1.        ]\n",
      " [1.        ]\n",
      " [1.        ]\n",
      " [1.        ]\n",
      " [1.        ]\n",
      " [1.        ]\n",
      " [1.        ]\n",
      " [1.        ]\n",
      " [1.        ]\n",
      " [1.        ]\n",
      " [1.        ]\n",
      " [1.        ]\n",
      " [0.9999916 ]\n",
      " [1.        ]\n",
      " [1.        ]\n",
      " [1.        ]\n",
      " [1.        ]\n",
      " [1.        ]\n",
      " [1.        ]\n",
      " [1.        ]\n",
      " [1.        ]\n",
      " [0.00630972]\n",
      " [1.        ]\n",
      " [1.        ]\n",
      " [1.        ]]\n",
      "Safe Network\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "model = tf.keras.models.load_model('tensorflow_model')\n",
    "data = pd.read_csv('attacks/Test/benign_4.csv')\n",
    "X_new = data[['Flow Bytes/s', 'Total Length of Fwd Packets', 'Fwd IAT Total', 'Flow Duration']]\n",
    "y_pred = model.predict(X_new)\n",
    "\n",
    "# Generating network health report\n",
    "print(\"\\n\\n\")\n",
    "print(110*\"*\")\n",
    "# print(f\"\\nReport For  File\\n\")\n",
    "print(y_pred)\n",
    "if np.mean(y_pred) < 0.5:\n",
    "    print(\"Malicious Network found, Disconnect quickly\")\n",
    "else:\n",
    "    print(\"Safe Network\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "00d23c19",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Found untraced functions such as _update_step_xla while saving (showing 1 of 1). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: /tmp/tmpcm3xy8pt/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: /tmp/tmpcm3xy8pt/assets\n",
      "2023-06-20 10:23:58.996628: W tensorflow/compiler/mlir/lite/python/tf_tfl_flatbuffer_helpers.cc:364] Ignored output_format.\n",
      "2023-06-20 10:23:58.996665: W tensorflow/compiler/mlir/lite/python/tf_tfl_flatbuffer_helpers.cc:367] Ignored drop_control_dependency.\n",
      "2023-06-20 10:23:58.997735: I tensorflow/cc/saved_model/reader.cc:45] Reading SavedModel from: /tmp/tmpcm3xy8pt\n",
      "2023-06-20 10:23:58.998385: I tensorflow/cc/saved_model/reader.cc:89] Reading meta graph with tags { serve }\n",
      "2023-06-20 10:23:58.998398: I tensorflow/cc/saved_model/reader.cc:130] Reading SavedModel debug info (if present) from: /tmp/tmpcm3xy8pt\n",
      "2023-06-20 10:23:59.000168: I tensorflow/compiler/mlir/mlir_graph_optimization_pass.cc:353] MLIR V1 optimization pass is not enabled\n",
      "2023-06-20 10:23:59.000747: I tensorflow/cc/saved_model/loader.cc:231] Restoring SavedModel bundle.\n",
      "2023-06-20 10:23:59.020399: I tensorflow/cc/saved_model/loader.cc:215] Running initialization op on SavedModel bundle at path: /tmp/tmpcm3xy8pt\n",
      "2023-06-20 10:23:59.026751: I tensorflow/cc/saved_model/loader.cc:314] SavedModel load for tags { serve }; Status: success: OK. Took 29018 microseconds.\n",
      "2023-06-20 10:23:59.062863: I tensorflow/compiler/mlir/tensorflow/utils/dump_mlir_util.cc:269] disabling MLIR crash reproducer, set env var `MLIR_CRASH_REPRODUCER_DIRECTORY` to enable.\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "# Load the saved TensorFlow model\n",
    "model = tf.keras.models.load_model('tensorflow_model')\n",
    "\n",
    "# Convert the TensorFlow model to TensorFlow Lite\n",
    "converter = tf.lite.TFLiteConverter.from_keras_model(model)\n",
    "tflite_model = converter.convert()\n",
    "\n",
    "# Save the TensorFlow Lite model\n",
    "with open('tensorflow_model.tflite', 'wb') as f:\n",
    "    f.write(tflite_model)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "704b961a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Use /tmp/tmpvv5uedub as temporary training directory\n",
      "Reading training dataset...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-06-20 11:21:12.387988: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'Placeholder/_1' with dtype int64 and shape [26]\n",
      "\t [[{{node Placeholder/_1}}]]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training dataset read in 0:00:03.355638. Found 26 examples.\n",
      "Training model...\n",
      "Model trained in 0:00:00.047682\n",
      "Compiling model...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[INFO 23-06-20 11:21:15.7715 IST kernel.cc:1242] Loading model from path /tmp/tmpvv5uedub/model/ with prefix cbbf342d17ae4379\n",
      "[WARNING 23-06-20 11:21:15.7741 IST utils.cc:73] The model does not have any input features i.e. the model is constant and will always return the same prediction.\n",
      "[INFO 23-06-20 11:21:15.7741 IST decision_forest.cc:660] Model loaded with 300 root(s), 300 node(s), and 0 input feature(s).\n",
      "[INFO 23-06-20 11:21:15.7742 IST abstract_model.cc:1311] Engine \"RandomForestOptPred\" built\n",
      "[INFO 23-06-20 11:21:15.7742 IST kernel.cc:1074] Use fast generic engine\n",
      "2023-06-20 11:21:15.803921: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'Placeholder/_1' with dtype int64 and shape [26]\n",
      "\t [[{{node Placeholder/_1}}]]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:AutoGraph could not transform <function simple_ml_inference_op_with_handle at 0x7f95509dac00> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: could not get source code\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:AutoGraph could not transform <function simple_ml_inference_op_with_handle at 0x7f95509dac00> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: could not get source code\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING: AutoGraph could not transform <function simple_ml_inference_op_with_handle at 0x7f95509dac00> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: could not get source code\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "Model compiled.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-06-20 11:21:18.490818: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'Placeholder/_1' with dtype int64 and shape [7]\n",
      "\t [[{{node Placeholder/_1}}]]\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "in user code:\n\n    File \"/home/abhi/.local/lib/python3.11/site-packages/tensorflow_decision_forests/keras/core_inference.py\", line 458, in run_step  *\n        outputs = model.predict_step(data)\n    File \"/home/abhi/.local/lib/python3.11/site-packages/keras/engine/training.py\", line 2111, in predict_step  **\n        return self(x, training=False)\n    File \"/home/abhi/.local/lib/python3.11/site-packages/keras/utils/traceback_utils.py\", line 70, in error_handler\n        raise e.with_traceback(filtered_tb) from None\n    File \"/tmp/__autograph_generated_filer7icmfzn.py\", line 104, in tf__call\n        ag__.if_stmt(ag__.ld(self)._semantics is None, if_body_2, else_body_2, get_state_3, set_state_3, ('do_return', 'retval_'), 2)\n    File \"/tmp/__autograph_generated_filer7icmfzn.py\", line 43, in else_body_2\n        normalized_inputs = ag__.converted_call(ag__.ld(self)._build_normalized_inputs, (ag__.ld(inputs),), None, fscope)\n    File \"/tmp/__autograph_generated_file_yqsmxqv.py\", line 99, in tf___build_normalized_inputs\n        normalized_semantic_inputs = ag__.converted_call(ag__.ld(tf_core).normalize_inputs, (ag__.ld(semantic_inputs),), dict(categorical_integer_offset_correction=ag__.not_(ag__.ld(self)._advanced_arguments.disable_categorical_integer_offset_correction)), fscope)\n    File \"/tmp/__autograph_generated_file6l7xp9fe.py\", line 205, in tf__normalize_inputs\n        ag__.for_stmt(ag__.converted_call(ag__.ld(inputs).items, (), None, fscope), None, loop_body, get_state_13, set_state_13, (), {'iterate_names': '(key, semantic_tensor)'})\n    File \"/tmp/__autograph_generated_file6l7xp9fe.py\", line 201, in loop_body\n        ag__.if_stmt(ag__.ld(semantic_tensor).semantic in [ag__.ld(Semantic).NUMERICAL, ag__.ld(Semantic).DISCRETIZED_NUMERICAL], if_body_12, else_body_12, get_state_12, set_state_12, ('normalized_inputs[key]',), 1)\n    File \"/tmp/__autograph_generated_file6l7xp9fe.py\", line 60, in if_body_12\n        ag__.if_stmt(ag__.ld(semantic_tensor).tensor.dtype in ag__.ld(FlexibleNumericalTypes), if_body, else_body, get_state, set_state, (), 0)\n    File \"/tmp/__autograph_generated_file6l7xp9fe.py\", line 56, in if_body\n        ag__.converted_call(ag__.ld(_unroll_and_normalize), (ag__.converted_call(ag__.ld(tf).cast, (ag__.ld(semantic_tensor).tensor, ag__.ld(tf).float32), None, fscope), ag__.ld(semantic_tensor).semantic, ag__.ld(key), ag__.ld(math).nan, ag__.ld(normalized_inputs)), None, fscope)\n    File \"/tmp/__autograph_generated_file8_kcjf4b.py\", line 28, in tf___unroll_and_normalize\n        rank = ag__.converted_call(ag__.ld(len), (ag__.ld(value).shape,), None, fscope)\n\n    ValueError: Exception encountered when calling layer 'random_forest_model_2' (type RandomForestModel).\n    \n    in user code:\n    \n        File \"/home/abhi/.local/lib/python3.11/site-packages/tensorflow_decision_forests/keras/core_inference.py\", line 644, in call  *\n            normalized_inputs = self._build_normalized_inputs(inputs)\n        File \"/home/abhi/.local/lib/python3.11/site-packages/tensorflow_decision_forests/keras/core_inference.py\", line 606, in _build_normalized_inputs  *\n            normalized_semantic_inputs = tf_core.normalize_inputs(\n        File \"/home/abhi/.local/lib/python3.11/site-packages/tensorflow_decision_forests/tensorflow/core_inference.py\", line 216, in normalize_inputs  *\n            _unroll_and_normalize(\n        File \"/home/abhi/.local/lib/python3.11/site-packages/tensorflow_decision_forests/tensorflow/core_inference.py\", line 354, in _unroll_and_normalize  *\n            rank = len(value.shape)\n    \n        ValueError: Cannot take the length of shape with unknown rank.\n    \n    \n    Call arguments received by layer 'random_forest_model_2' (type RandomForestModel):\n      • inputs=tf.Tensor(shape=(4,), dtype=float32)\n      • training=False\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn [21], line 36\u001b[0m\n\u001b[1;32m     33\u001b[0m model\u001b[38;5;241m.\u001b[39mfit(train_data)\n\u001b[1;32m     35\u001b[0m \u001b[38;5;66;03m# Evaluate the model\u001b[39;00m\n\u001b[0;32m---> 36\u001b[0m y_pred \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpredict\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtest_data\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     37\u001b[0m y_pred_binary \u001b[38;5;241m=\u001b[39m [\u001b[38;5;241m1\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m pred \u001b[38;5;241m>\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0.5\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;241m0\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m pred \u001b[38;5;129;01min\u001b[39;00m y_pred]\n\u001b[1;32m     39\u001b[0m accuracy \u001b[38;5;241m=\u001b[39m model\u001b[38;5;241m.\u001b[39mevaluate(test_data)[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124maccuracy\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n",
      "File \u001b[0;32m~/.local/lib/python3.11/site-packages/keras/utils/traceback_utils.py:70\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     67\u001b[0m     filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n\u001b[1;32m     68\u001b[0m     \u001b[38;5;66;03m# To get the full stack trace, call:\u001b[39;00m\n\u001b[1;32m     69\u001b[0m     \u001b[38;5;66;03m# `tf.debugging.disable_traceback_filtering()`\u001b[39;00m\n\u001b[0;32m---> 70\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m e\u001b[38;5;241m.\u001b[39mwith_traceback(filtered_tb) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28mNone\u001b[39m\n\u001b[1;32m     71\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m     72\u001b[0m     \u001b[38;5;28;01mdel\u001b[39;00m filtered_tb\n",
      "File \u001b[0;32m/tmp/__autograph_generated_file7gi9v6s6.py:43\u001b[0m, in \u001b[0;36mouter_factory.<locals>.inner_factory.<locals>.tf__predict_function_trained\u001b[0;34m(iterator, model)\u001b[0m\n\u001b[1;32m     41\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m     42\u001b[0m     do_return \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[0;32m---> 43\u001b[0m     retval_ \u001b[38;5;241m=\u001b[39m ag__\u001b[38;5;241m.\u001b[39mconverted_call(ag__\u001b[38;5;241m.\u001b[39mld(run_step), (ag__\u001b[38;5;241m.\u001b[39mld(data),), \u001b[38;5;28;01mNone\u001b[39;00m, fscope)\n\u001b[1;32m     44\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m:\n\u001b[1;32m     45\u001b[0m     do_return \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n",
      "File \u001b[0;32m/tmp/__autograph_generated_file7gi9v6s6.py:30\u001b[0m, in \u001b[0;36mouter_factory.<locals>.inner_factory.<locals>.tf__predict_function_trained.<locals>.run_step\u001b[0;34m(data)\u001b[0m\n\u001b[1;32m     28\u001b[0m do_return_1 \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[1;32m     29\u001b[0m retval__1 \u001b[38;5;241m=\u001b[39m ag__\u001b[38;5;241m.\u001b[39mUndefinedReturnValue()\n\u001b[0;32m---> 30\u001b[0m outputs \u001b[38;5;241m=\u001b[39m \u001b[43mag__\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconverted_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43mag__\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mld\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpredict_step\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m(\u001b[49m\u001b[43mag__\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mld\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfscope_1\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     31\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m ag__\u001b[38;5;241m.\u001b[39mld(tf)\u001b[38;5;241m.\u001b[39mcontrol_dependencies(ag__\u001b[38;5;241m.\u001b[39mld(_minimum_control_deps)(ag__\u001b[38;5;241m.\u001b[39mld(outputs))):\n\u001b[1;32m     32\u001b[0m     ag__\u001b[38;5;241m.\u001b[39mconverted_call(ag__\u001b[38;5;241m.\u001b[39mld(model)\u001b[38;5;241m.\u001b[39m_predict_counter\u001b[38;5;241m.\u001b[39massign_add, (\u001b[38;5;241m1\u001b[39m,), \u001b[38;5;28;01mNone\u001b[39;00m, fscope_1)\n",
      "File \u001b[0;32m/tmp/__autograph_generated_filer7icmfzn.py:104\u001b[0m, in \u001b[0;36mouter_factory.<locals>.inner_factory.<locals>.tf__call\u001b[0;34m(self, inputs, training)\u001b[0m\n\u001b[1;32m    102\u001b[0m multitask_item \u001b[38;5;241m=\u001b[39m ag__\u001b[38;5;241m.\u001b[39mUndefined(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmultitask_item\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m    103\u001b[0m normalized_inputs \u001b[38;5;241m=\u001b[39m ag__\u001b[38;5;241m.\u001b[39mUndefined(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mnormalized_inputs\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m--> 104\u001b[0m \u001b[43mag__\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mif_stmt\u001b[49m\u001b[43m(\u001b[49m\u001b[43mag__\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mld\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_semantics\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mis\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mif_body_2\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43melse_body_2\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mget_state_3\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mset_state_3\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mdo_return\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mretval_\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m2\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m    105\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m fscope\u001b[38;5;241m.\u001b[39mret(retval_, do_return)\n",
      "File \u001b[0;32m/tmp/__autograph_generated_filer7icmfzn.py:43\u001b[0m, in \u001b[0;36mouter_factory.<locals>.inner_factory.<locals>.tf__call.<locals>.else_body_2\u001b[0;34m()\u001b[0m\n\u001b[1;32m     41\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21melse_body_2\u001b[39m():\n\u001b[1;32m     42\u001b[0m     \u001b[38;5;28;01mnonlocal\u001b[39;00m do_return, retval_\n\u001b[0;32m---> 43\u001b[0m     normalized_inputs \u001b[38;5;241m=\u001b[39m \u001b[43mag__\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconverted_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43mag__\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mld\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_build_normalized_inputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m(\u001b[49m\u001b[43mag__\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mld\u001b[49m\u001b[43m(\u001b[49m\u001b[43minputs\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfscope\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     44\u001b[0m     predictions \u001b[38;5;241m=\u001b[39m {}\n\u001b[1;32m     46\u001b[0m     \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mget_state\u001b[39m():\n",
      "File \u001b[0;32m/tmp/__autograph_generated_file_yqsmxqv.py:99\u001b[0m, in \u001b[0;36mouter_factory.<locals>.inner_factory.<locals>.tf___build_normalized_inputs\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m     97\u001b[0m ag__\u001b[38;5;241m.\u001b[39mif_stmt(ag__\u001b[38;5;241m.\u001b[39mconverted_call(ag__\u001b[38;5;241m.\u001b[39mld(\u001b[38;5;28misinstance\u001b[39m), (ag__\u001b[38;5;241m.\u001b[39mld(inputs), ag__\u001b[38;5;241m.\u001b[39mld(\u001b[38;5;28mdict\u001b[39m)), \u001b[38;5;28;01mNone\u001b[39;00m, fscope), if_body_4, else_body_4, get_state_4, set_state_4, (\u001b[38;5;124m'\u001b[39m\u001b[38;5;124minputs\u001b[39m\u001b[38;5;124m'\u001b[39m,), \u001b[38;5;241m1\u001b[39m)\n\u001b[1;32m     98\u001b[0m semantic_inputs \u001b[38;5;241m=\u001b[39m ag__\u001b[38;5;241m.\u001b[39mconverted_call(ag__\u001b[38;5;241m.\u001b[39mld(tf_core)\u001b[38;5;241m.\u001b[39mcombine_tensors_and_semantics, (ag__\u001b[38;5;241m.\u001b[39mld(inputs), ag__\u001b[38;5;241m.\u001b[39mld(\u001b[38;5;28mself\u001b[39m)\u001b[38;5;241m.\u001b[39m_semantics), \u001b[38;5;28;01mNone\u001b[39;00m, fscope)\n\u001b[0;32m---> 99\u001b[0m normalized_semantic_inputs \u001b[38;5;241m=\u001b[39m \u001b[43mag__\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconverted_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43mag__\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mld\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtf_core\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnormalize_inputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m(\u001b[49m\u001b[43mag__\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mld\u001b[49m\u001b[43m(\u001b[49m\u001b[43msemantic_inputs\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mdict\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mcategorical_integer_offset_correction\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mag__\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnot_\u001b[49m\u001b[43m(\u001b[49m\u001b[43mag__\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mld\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_advanced_arguments\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdisable_categorical_integer_offset_correction\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfscope\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    100\u001b[0m normalized_inputs, _ \u001b[38;5;241m=\u001b[39m ag__\u001b[38;5;241m.\u001b[39mconverted_call(ag__\u001b[38;5;241m.\u001b[39mld(tf_core)\u001b[38;5;241m.\u001b[39mdecombine_tensors_and_semantics, (ag__\u001b[38;5;241m.\u001b[39mld(normalized_semantic_inputs),), \u001b[38;5;28;01mNone\u001b[39;00m, fscope)\n\u001b[1;32m    101\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n",
      "File \u001b[0;32m/tmp/__autograph_generated_file6l7xp9fe.py:205\u001b[0m, in \u001b[0;36mouter_factory.<locals>.inner_factory.<locals>.tf__normalize_inputs\u001b[0;34m(inputs, categorical_integer_offset_correction)\u001b[0m\n\u001b[1;32m    203\u001b[0m semantic_tensor \u001b[38;5;241m=\u001b[39m ag__\u001b[38;5;241m.\u001b[39mUndefined(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124msemantic_tensor\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m    204\u001b[0m key \u001b[38;5;241m=\u001b[39m ag__\u001b[38;5;241m.\u001b[39mUndefined(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mkey\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m--> 205\u001b[0m \u001b[43mag__\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfor_stmt\u001b[49m\u001b[43m(\u001b[49m\u001b[43mag__\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconverted_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43mag__\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mld\u001b[49m\u001b[43m(\u001b[49m\u001b[43minputs\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mitems\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfscope\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mloop_body\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mget_state_13\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mset_state_13\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m{\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43miterate_names\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43m(key, semantic_tensor)\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m}\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    206\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m    207\u001b[0m     do_return \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n",
      "File \u001b[0;32m/tmp/__autograph_generated_file6l7xp9fe.py:201\u001b[0m, in \u001b[0;36mouter_factory.<locals>.inner_factory.<locals>.tf__normalize_inputs.<locals>.loop_body\u001b[0;34m(itr)\u001b[0m\n\u001b[1;32m    199\u001b[0m         ag__\u001b[38;5;241m.\u001b[39mif_stmt(ag__\u001b[38;5;241m.\u001b[39mld(semantic_tensor)\u001b[38;5;241m.\u001b[39msemantic \u001b[38;5;241m==\u001b[39m ag__\u001b[38;5;241m.\u001b[39mld(Semantic)\u001b[38;5;241m.\u001b[39mCATEGORICAL_SET, if_body_10, else_body_10, get_state_10, set_state_10, (\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mnormalized_inputs[key]\u001b[39m\u001b[38;5;124m'\u001b[39m,), \u001b[38;5;241m1\u001b[39m)\n\u001b[1;32m    200\u001b[0m     ag__\u001b[38;5;241m.\u001b[39mif_stmt(ag__\u001b[38;5;241m.\u001b[39mld(semantic_tensor)\u001b[38;5;241m.\u001b[39msemantic \u001b[38;5;241m==\u001b[39m ag__\u001b[38;5;241m.\u001b[39mld(Semantic)\u001b[38;5;241m.\u001b[39mCATEGORICAL, if_body_11, else_body_11, get_state_11, set_state_11, (\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mnormalized_inputs[key]\u001b[39m\u001b[38;5;124m'\u001b[39m,), \u001b[38;5;241m1\u001b[39m)\n\u001b[0;32m--> 201\u001b[0m \u001b[43mag__\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mif_stmt\u001b[49m\u001b[43m(\u001b[49m\u001b[43mag__\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mld\u001b[49m\u001b[43m(\u001b[49m\u001b[43msemantic_tensor\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msemantic\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43m[\u001b[49m\u001b[43mag__\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mld\u001b[49m\u001b[43m(\u001b[49m\u001b[43mSemantic\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mNUMERICAL\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mag__\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mld\u001b[49m\u001b[43m(\u001b[49m\u001b[43mSemantic\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mDISCRETIZED_NUMERICAL\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mif_body_12\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43melse_body_12\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mget_state_12\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mset_state_12\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mnormalized_inputs[key]\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/tmp/__autograph_generated_file6l7xp9fe.py:60\u001b[0m, in \u001b[0;36mouter_factory.<locals>.inner_factory.<locals>.tf__normalize_inputs.<locals>.loop_body.<locals>.if_body_12\u001b[0;34m()\u001b[0m\n\u001b[1;32m     58\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21melse_body\u001b[39m():\n\u001b[1;32m     59\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m ag__\u001b[38;5;241m.\u001b[39mconverted_call(ag__\u001b[38;5;241m.\u001b[39mld(\u001b[38;5;167;01mValueError\u001b[39;00m), (ag__\u001b[38;5;241m.\u001b[39mconverted_call(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mNon supported tensor dtype \u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m for semantic \u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m of feature \u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m.\u001b[39mformat, (ag__\u001b[38;5;241m.\u001b[39mld(semantic_tensor)\u001b[38;5;241m.\u001b[39mtensor\u001b[38;5;241m.\u001b[39mdtype, ag__\u001b[38;5;241m.\u001b[39mld(semantic_tensor)\u001b[38;5;241m.\u001b[39msemantic, ag__\u001b[38;5;241m.\u001b[39mld(key)), \u001b[38;5;28;01mNone\u001b[39;00m, fscope),), \u001b[38;5;28;01mNone\u001b[39;00m, fscope)\n\u001b[0;32m---> 60\u001b[0m \u001b[43mag__\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mif_stmt\u001b[49m\u001b[43m(\u001b[49m\u001b[43mag__\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mld\u001b[49m\u001b[43m(\u001b[49m\u001b[43msemantic_tensor\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtensor\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdtype\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mag__\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mld\u001b[49m\u001b[43m(\u001b[49m\u001b[43mFlexibleNumericalTypes\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mif_body\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43melse_body\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mget_state\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mset_state\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/tmp/__autograph_generated_file6l7xp9fe.py:56\u001b[0m, in \u001b[0;36mouter_factory.<locals>.inner_factory.<locals>.tf__normalize_inputs.<locals>.loop_body.<locals>.if_body_12.<locals>.if_body\u001b[0;34m()\u001b[0m\n\u001b[1;32m     55\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mif_body\u001b[39m():\n\u001b[0;32m---> 56\u001b[0m     \u001b[43mag__\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconverted_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43mag__\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mld\u001b[49m\u001b[43m(\u001b[49m\u001b[43m_unroll_and_normalize\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m(\u001b[49m\u001b[43mag__\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconverted_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43mag__\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mld\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtf\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcast\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m(\u001b[49m\u001b[43mag__\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mld\u001b[49m\u001b[43m(\u001b[49m\u001b[43msemantic_tensor\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtensor\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mag__\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mld\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtf\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfloat32\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfscope\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mag__\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mld\u001b[49m\u001b[43m(\u001b[49m\u001b[43msemantic_tensor\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msemantic\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mag__\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mld\u001b[49m\u001b[43m(\u001b[49m\u001b[43mkey\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mag__\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mld\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmath\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnan\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mag__\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mld\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnormalized_inputs\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfscope\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/tmp/__autograph_generated_file8_kcjf4b.py:28\u001b[0m, in \u001b[0;36mouter_factory.<locals>.inner_factory.<locals>.tf___unroll_and_normalize\u001b[0;34m(value, semantic, base_key, missing_value, output, dense_preprocess)\u001b[0m\n\u001b[1;32m     26\u001b[0m     \u001b[38;5;28;01mpass\u001b[39;00m\n\u001b[1;32m     27\u001b[0m ag__\u001b[38;5;241m.\u001b[39mif_stmt(ag__\u001b[38;5;241m.\u001b[39mld(dense_preprocess), if_body, else_body, get_state, set_state, (\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mvalue\u001b[39m\u001b[38;5;124m'\u001b[39m,), \u001b[38;5;241m1\u001b[39m)\n\u001b[0;32m---> 28\u001b[0m rank \u001b[38;5;241m=\u001b[39m \u001b[43mag__\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconverted_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43mag__\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mld\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mlen\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m(\u001b[49m\u001b[43mag__\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mld\u001b[49m\u001b[43m(\u001b[49m\u001b[43mvalue\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mshape\u001b[49m\u001b[43m,\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfscope\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     30\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mget_state_5\u001b[39m():\n\u001b[1;32m     31\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m (do_return, ag__\u001b[38;5;241m.\u001b[39mldu(\u001b[38;5;28;01mlambda\u001b[39;00m: output[base_key], \u001b[38;5;124m'\u001b[39m\u001b[38;5;124moutput[base_key]\u001b[39m\u001b[38;5;124m'\u001b[39m), retval_)\n",
      "\u001b[0;31mValueError\u001b[0m: in user code:\n\n    File \"/home/abhi/.local/lib/python3.11/site-packages/tensorflow_decision_forests/keras/core_inference.py\", line 458, in run_step  *\n        outputs = model.predict_step(data)\n    File \"/home/abhi/.local/lib/python3.11/site-packages/keras/engine/training.py\", line 2111, in predict_step  **\n        return self(x, training=False)\n    File \"/home/abhi/.local/lib/python3.11/site-packages/keras/utils/traceback_utils.py\", line 70, in error_handler\n        raise e.with_traceback(filtered_tb) from None\n    File \"/tmp/__autograph_generated_filer7icmfzn.py\", line 104, in tf__call\n        ag__.if_stmt(ag__.ld(self)._semantics is None, if_body_2, else_body_2, get_state_3, set_state_3, ('do_return', 'retval_'), 2)\n    File \"/tmp/__autograph_generated_filer7icmfzn.py\", line 43, in else_body_2\n        normalized_inputs = ag__.converted_call(ag__.ld(self)._build_normalized_inputs, (ag__.ld(inputs),), None, fscope)\n    File \"/tmp/__autograph_generated_file_yqsmxqv.py\", line 99, in tf___build_normalized_inputs\n        normalized_semantic_inputs = ag__.converted_call(ag__.ld(tf_core).normalize_inputs, (ag__.ld(semantic_inputs),), dict(categorical_integer_offset_correction=ag__.not_(ag__.ld(self)._advanced_arguments.disable_categorical_integer_offset_correction)), fscope)\n    File \"/tmp/__autograph_generated_file6l7xp9fe.py\", line 205, in tf__normalize_inputs\n        ag__.for_stmt(ag__.converted_call(ag__.ld(inputs).items, (), None, fscope), None, loop_body, get_state_13, set_state_13, (), {'iterate_names': '(key, semantic_tensor)'})\n    File \"/tmp/__autograph_generated_file6l7xp9fe.py\", line 201, in loop_body\n        ag__.if_stmt(ag__.ld(semantic_tensor).semantic in [ag__.ld(Semantic).NUMERICAL, ag__.ld(Semantic).DISCRETIZED_NUMERICAL], if_body_12, else_body_12, get_state_12, set_state_12, ('normalized_inputs[key]',), 1)\n    File \"/tmp/__autograph_generated_file6l7xp9fe.py\", line 60, in if_body_12\n        ag__.if_stmt(ag__.ld(semantic_tensor).tensor.dtype in ag__.ld(FlexibleNumericalTypes), if_body, else_body, get_state, set_state, (), 0)\n    File \"/tmp/__autograph_generated_file6l7xp9fe.py\", line 56, in if_body\n        ag__.converted_call(ag__.ld(_unroll_and_normalize), (ag__.converted_call(ag__.ld(tf).cast, (ag__.ld(semantic_tensor).tensor, ag__.ld(tf).float32), None, fscope), ag__.ld(semantic_tensor).semantic, ag__.ld(key), ag__.ld(math).nan, ag__.ld(normalized_inputs)), None, fscope)\n    File \"/tmp/__autograph_generated_file8_kcjf4b.py\", line 28, in tf___unroll_and_normalize\n        rank = ag__.converted_call(ag__.ld(len), (ag__.ld(value).shape,), None, fscope)\n\n    ValueError: Exception encountered when calling layer 'random_forest_model_2' (type RandomForestModel).\n    \n    in user code:\n    \n        File \"/home/abhi/.local/lib/python3.11/site-packages/tensorflow_decision_forests/keras/core_inference.py\", line 644, in call  *\n            normalized_inputs = self._build_normalized_inputs(inputs)\n        File \"/home/abhi/.local/lib/python3.11/site-packages/tensorflow_decision_forests/keras/core_inference.py\", line 606, in _build_normalized_inputs  *\n            normalized_semantic_inputs = tf_core.normalize_inputs(\n        File \"/home/abhi/.local/lib/python3.11/site-packages/tensorflow_decision_forests/tensorflow/core_inference.py\", line 216, in normalize_inputs  *\n            _unroll_and_normalize(\n        File \"/home/abhi/.local/lib/python3.11/site-packages/tensorflow_decision_forests/tensorflow/core_inference.py\", line 354, in _unroll_and_normalize  *\n            rank = len(value.shape)\n    \n        ValueError: Cannot take the length of shape with unknown rank.\n    \n    \n    Call arguments received by layer 'random_forest_model_2' (type RandomForestModel):\n      • inputs=tf.Tensor(shape=(4,), dtype=float32)\n      • training=False\n"
     ]
    }
   ],
   "source": [
    "import tensorflow_decision_forests as tfdf\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import f1_score, precision_score, recall_score\n",
    "import joblib\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "\n",
    "# Load the dataset\n",
    "data = pd.read_csv('portScan_test.csv')\n",
    "X = data[['Flow Bytes/s', 'Total Length of Fwd Packets', 'Fwd IAT Total', 'Flow Duration']]\n",
    "\n",
    "attack_or_not=[]\n",
    "for i in data['Label']:\n",
    "    if i ==\"BENIGN\":\n",
    "        attack_or_not.append(1)\n",
    "    else:\n",
    "        attack_or_not.append(0)     \n",
    "data['Label'] = attack_or_not\n",
    "\n",
    "y = data['Label']\n",
    "\n",
    "# Split the data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Convert the data to TensorFlow Dataset format and add a batch operation\n",
    "train_data = tf.data.Dataset.from_tensor_slices((X_train.values, y_train.values)).batch(32)\n",
    "test_data = tf.data.Dataset.from_tensor_slices((X_test.values, y_test.values))\n",
    "\n",
    "# Define the TF-DF model\n",
    "model = tfdf.keras.RandomForestModel(task=tfdf.keras.Task.CLASSIFICATION)\n",
    "\n",
    "# Train the model\n",
    "model.fit(train_data)\n",
    "\n",
    "# Evaluate the model\n",
    "y_pred = model.predict(test_data)\n",
    "y_pred_binary = [1 if pred >= 0.5 else 0 for pred in y_pred]\n",
    "\n",
    "accuracy = model.evaluate(test_data)[\"accuracy\"]\n",
    "precision = precision_score(y_test, y_pred_binary)\n",
    "recall = recall_score(y_test, y_pred_binary)\n",
    "f1 = f1_score(y_test, y_pred_binary)\n",
    "\n",
    "# Print the evaluation metrics\n",
    "print(\"Accuracy:\", accuracy)\n",
    "print(\"Precision:\", precision)\n",
    "print(\"Recall:\", recall)\n",
    "print(\"F1-Score:\", f1)\n",
    "\n",
    "# Save the model\n",
    "model.save('tfdf_model')\n",
    "\n",
    "# Save the metrics\n",
    "metrics = {'accuracy': accuracy, 'precision': precision, 'recall': recall, 'f1-score': f1}\n",
    "joblib.dump(metrics, 'metrics.pkl')\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
